{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Jozef Porubcin\n",
        "# Semester Project\n",
        "# Code adapted from Practical 2: Skeleton credit to Yifan Qin, Zheng Ning, and Adam Czajka"
      ],
      "metadata": {
        "id": "GloO83atiCi7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "bK6wHaMsO3Pg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.transforms.functional import resize\n",
        "from torchvision.transforms import CenterCrop\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import read_image\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import csv\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uS8Cxsgp1aaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9d6735-4baa-4b6a-ad7f-3a84b068c6e4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, numChannels, numClasses):\n",
        "        super(CNN, self).__init__()\n",
        "        self.classes = numClasses\n",
        "        \n",
        "        conv_out1 = 96\n",
        "        conv_out2 = 256\n",
        "        conv_out3 = 384\n",
        "        conv_out4 = 384\n",
        "        conv_out5 = 256\n",
        "        self.conv1 = torch.nn.Conv2d(numChannels, conv_out1, (11, 11), (4, 4))\n",
        "        self.conv2 = torch.nn.Conv2d(conv_out1, conv_out2, (5, 5), (1, 1))\n",
        "        self.conv3 = torch.nn.Conv2d(conv_out2, conv_out3, (3, 3), (1, 1))\n",
        "        self.conv4 = torch.nn.Conv2d(conv_out3, conv_out4, (3, 3), (1, 1))\n",
        "        self.conv5 = torch.nn.Conv2d(conv_out4, conv_out5, (3, 3), (1, 1))\n",
        "\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "        self.maxpool = torch.nn.MaxPool2d((2, 2), (2, 2))\n",
        "\n",
        "        self.batchnorm1 = torch.nn.BatchNorm2d(conv_out1)\n",
        "        self.batchnorm2 = torch.nn.BatchNorm2d(conv_out2)\n",
        "\n",
        "        fc1_out1 = 1024\n",
        "        self.fc1 = torch.nn.Linear(conv_out5*15, fc1_out1)\n",
        "        self.fc2 = torch.nn.Linear(fc1_out1, numClasses)\n",
        "\n",
        "    def evaluate(self, model, dataloader, classes, device):\n",
        "\n",
        "        # We need to switch the model into the evaluation mode\n",
        "        model.eval()\n",
        "        \n",
        "        correct_pred = {classname: 0 for classname in classes}\n",
        "        total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "            images = images.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                \n",
        "                # If you want to see real and predicted labels for all samples:\n",
        "                # print(\"Real class: \" + classes[label] + \", predicted = \" + classes[prediction])\n",
        "                \n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "        acc = sum(correct_pred.values()) / sum(total_pred.values())\n",
        "\n",
        "        return acc\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = resize(x, size=[256])\n",
        "\n",
        "        # First convolutional block\n",
        "        # self.conv1 -> self.relu -> self.maxpool -> self.batchnorm1\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.batchnorm1(x)\n",
        "\n",
        "        # Second convolutional block\n",
        "        # self.conv2 -> self.relu -> self.maxpool -> self.batchnorm2\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.batchnorm2(x)\n",
        "\n",
        "        # Third convolutional block\n",
        "        # self.conv3 -> self.relu\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Fourth convolutional block\n",
        "        # self.conv4 -> self.relu\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Fifth convolutional block\n",
        "        # self.conv5 -> self.relu -> self.maxpool\n",
        "        x = self.conv5(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "\n",
        "        # After the last pooling operation, and before the first \n",
        "        # fully-connected layer, we need to \"flatten\" our tensors\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "        # self.fc1 -> self.relu -> self.fc2\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "L_EExNrR1dAn"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which layers are \"trainable\"\n",
        "def check(model):\n",
        "    print(\"Layers to be fine-tuned:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(name)\n"
      ],
      "metadata": {
        "id": "WA1O65p4M_dM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FilteredFruits(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, transform):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_labels = pd.read_csv(label_dir, header=0)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        image = image.to(torch.float) / 256.\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image, label)"
      ],
      "metadata": {
        "id": "ufBf3GuXNC64"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_labels(image_dir, label_dir):\n",
        "    row_list = [['file', 'label']]\n",
        "\n",
        "    min_images = float('inf')\n",
        "    for class_dir in os.scandir(image_dir):\n",
        "        if class_dir.is_dir():\n",
        "            min_images = min(len(os.listdir(class_dir)), min_images)\n",
        "\n",
        "    l = 0\n",
        "    for class_dir in os.scandir(image_dir):\n",
        "        if class_dir.is_dir():\n",
        "            img_files = os.listdir(class_dir)\n",
        "            random.shuffle(img_files)\n",
        "            for x in range(min_images):\n",
        "                row_list.append([(class_dir.name + '/' + img_files[x]), l])\n",
        "\n",
        "            l += 1\n",
        "    \n",
        "    with open(label_dir, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(row_list)"
      ],
      "metadata": {
        "id": "bjwS8s_DiOFW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Transform images:\n",
        "    # a) to tensor: convert the PIL image or numpy.ndarray to tensor\n",
        "    # b) Z-normalize a tensor image (using its mean and standard deviation)\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    \n",
        "    # Path to the fruits dataset:\n",
        "    image_dir = '/content/drive/MyDrive/Semester_Project/208x256/'\n",
        "\n",
        "    # Path to the labels of the sampled CIFAR-100 subset\n",
        "    label_dir = '/content/drive/MyDrive/Semester_Project/208x256/fruits_labels.csv'\n",
        "\n",
        "    generate_labels(image_dir, label_dir)\n",
        "\n",
        "    transfer_learning = 'all_layers'\n",
        "\n",
        "    epochs = 50\n",
        "    batch_size = 20\n",
        "\n",
        "    # Load sampled fruits dataset\n",
        "    data = FilteredFruits(img_dir=image_dir, label_dir=label_dir, transform=transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "    train_len = int(len(data) * 0.8)\n",
        "    val_len = int(len(data) * 0.1)\n",
        "    test_len = int(len(data) - train_len - val_len)\n",
        "    train_data, val_data, test_data = random_split(data, [train_len, val_len, test_len])\n",
        "    classes = ['apple', 'banana', 'black berry', 'blueberry', 'cherry', 'grape', 'mango', 'orange',  'pear', 'pineapple', 'pineberry', 'pomegranate', 'raspberry', 'strawberry', 'watermelon']\n",
        "\n",
        "    # Prepare data loaders for train, validation and test data splits \n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2)\n",
        "\n",
        "    #######################################################\n",
        "    # Specify the operation mode:\n",
        "    # 'train' = training with your train and validation data splits\n",
        "    # 'eval'  = evaluation of the trained model with your test data split\n",
        "    mode = 'eval'\n",
        "\n",
        "    # Path where you will save the best model after fine-tuning\n",
        "    my_best_model = \"/content/drive/MyDrive/Semester_Project/best_model.pth\"\n",
        "\n",
        "    #\n",
        "    #######################################################\n",
        "\n",
        "\n",
        "    # Set the device (GPU or CPU, depending on availability)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Currently using device: \", device)\n",
        "\n",
        "    # Initialize the model\n",
        "    model_t = CNN(numChannels = 3, numClasses = len(classes))\n",
        "    model_t.to(device)\n",
        "    \n",
        "    # Training happens here!\n",
        "    if mode == \"train\":\n",
        "\n",
        "        print(\"\\n\\nFine-tuning (transfer learning) starts!\\n\\n\")\n",
        "\n",
        "        model_t.train()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        #optimizer = torch.optim.Adam(model_t.parameters(), lr=0.001)\n",
        "        optimizer = torch.optim.SGD(model_t.parameters(), lr=0.001, momentum=0.9)\n",
        "        '''\n",
        "        if my_starting_weights and transfer_learning != \"none\":\n",
        "            print(f\"Loading the weights from {my_starting_weights} ...\")\n",
        "            model_t.load_state_dict(torch.load(my_starting_weights))\n",
        "            print(\"Successfully loaded the model checkpoint!\")\n",
        "        '''\n",
        "        # Sanity check: print layers that are being fine-tuned (not frozen)\n",
        "        print(check(model_t))\n",
        "\n",
        "        running_loss = .0\n",
        "        best_acc = .0\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Starting epoch {epoch + 1}\")\n",
        "            for idx, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "\n",
        "                # Get the inputs (data is a list of [inputs, labels])\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model_t(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                loss = loss.detach().cpu().numpy()\n",
        "                inputs = inputs.detach().cpu().numpy()\n",
        "                labels = labels.detach().cpu().numpy()\n",
        "                running_loss += loss\n",
        "\n",
        "            # Evaluate the accuracy after each epoch\n",
        "            acc = model_t.evaluate(model_t, val_loader, classes, device)\n",
        "            if acc > best_acc:\n",
        "                print(f\"Better validation accuracy achieved: {acc * 100:.2f}%\")\n",
        "                best_acc = acc\n",
        "                print(f\"Saving this model as: {my_best_model}\")\n",
        "                torch.save(model_t.state_dict(), my_best_model)\n",
        "\n",
        "    # And here we evaluate the trained model with the test data\n",
        "    elif mode == \"eval\":\n",
        "        print(f\"Loading checkpoint from {my_best_model}\")\n",
        "        model_t.load_state_dict(torch.load(my_best_model))\n",
        "        acc = model_t.evaluate(model_t, test_loader, classes, device)\n",
        "        print(f\"Accuracy on the test (unknown) data: {acc * 100:.2f}%\")\n",
        "\n",
        "    else:\n",
        "        print(\"'mode' argument should either be 'train' or 'eval'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rr6pc-n9OlGy",
        "outputId": "b647d0ec-6f4e-48e3-9e1f-8967cac24e09"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently using device:  cuda:0\n",
            "Loading checkpoint from /content/drive/MyDrive/Semester_Project/best_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test (unknown) data: 18.94%\n"
          ]
        }
      ]
    }
  ]
}